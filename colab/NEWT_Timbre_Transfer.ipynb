{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEWT Timbre Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben-hayes/neural-waveshaping-synthesis/blob/main/colab/NEWT_Timbre_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZjIcyWSY5MW"
      },
      "source": [
        "<h1 align=\"center\">neural waveshaping synthesis</h1>\n",
        "<h4 align=\"center\">real-time neural audio synthesis in the waveform domain</h4>\n",
        "\n",
        "<div align=\"center\">\n",
        "<h4>\n",
        "    <a href=\"https://benhayes.net/assets/pdf/nws_arxiv.pdf\" target=\"_blank\">paper</a> •\n",
        "        <a href=\"https://benhayes.net/projects/nws/\" target=\"_blank\">website</a> • \n",
        "        <a href=\"https://github.com/ben-hayes/neural-waveshaping-synthesis/\" target=\"_blank\">github</a> • \n",
        "        <a href=\"https://benhayes.net/projects/nws/#audio-examples\">audio</a>\n",
        "    </h4>\n",
        "    <p>\n",
        "    by <em>Ben Hayes, Charalampos Saitis, György Fazekas</em>\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<p>\n",
        "This Google Colab notebook is a supplement to our ISMIR 2021 paper, <i>Neural Waveshaping Synthesis</i>.\n",
        "By running the code in the following cells, you can use one of our pretrained models to perform timbre transfer from any monophonic audio to flute, trumpet, or violin!\n",
        "</p>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://benhayes.net/assets/img/newt_shapers.png\" /></p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSGz_-2IOMIr"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoJjEMHQuviu",
        "cellView": "form"
      },
      "source": [
        "%%capture\n",
        "#@title Install dependencies\n",
        "#@markdown \n",
        "!rm -rf neural-waveshaping-synthesis\n",
        "!git clone https://github.com/ben-hayes/neural-waveshaping-synthesis.git\n",
        "%cd neural-waveshaping-synthesis\n",
        "!pip install youtube-dl pytorch-lightning auraloss==0.2.1 librosa==0.8.0 torchcrepe==0.0.12 wandb\n",
        "!pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XRR5iINku3LA",
        "cellView": "form"
      },
      "source": [
        "%%capture\n",
        "#@title Make imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from dl_colab_notebooks.audio import record_audio\n",
        "import gin\n",
        "from google.colab import files\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import torch\n",
        "\n",
        "from neural_waveshaping_synthesis.data.utils.loudness_extraction import extract_perceptual_loudness\n",
        "from neural_waveshaping_synthesis.data.utils.mfcc_extraction import extract_mfcc\n",
        "from neural_waveshaping_synthesis.data.utils.f0_extraction import extract_f0_with_crepe\n",
        "from neural_waveshaping_synthesis.data.utils.preprocess_audio import preprocess_audio, convert_to_float32_audio, make_monophonic, resample_audio\n",
        "from neural_waveshaping_synthesis.models.neural_waveshaping import NeuralWaveshaping\n",
        "\n",
        "try:\n",
        "  gin.constant(\"device\", \"cuda\")\n",
        "except ValueError as err:\n",
        "  pass\n",
        "gin.parse_config_file(\"gin/models/newt.gin\")\n",
        "gin.parse_config_file(\"gin/data/urmp_4second_crepe.gin\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "checkpoints = dict(Violin=\"vn\", Flute=\"fl\", Trumpet=\"tpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HqDjn-7V2v",
        "cellView": "form"
      },
      "source": [
        "%%capture\n",
        "#@title Load Checkpoint { run: \"auto\" }\n",
        "#@markdown Choose from one of three pretrained checkpoints. In future you will be able to upload your own checkpoints too.\n",
        "selected_checkpoint_name = \"Violin\" #@param [\"Violin\", \"Flute\", \"Trumpet\"]\n",
        "selected_checkpoint = checkpoints[selected_checkpoint_name]\n",
        "\n",
        "checkpoint_path = os.path.join(\n",
        "    \"checkpoints/nws\", selected_checkpoint)\n",
        "model = NeuralWaveshaping.load_from_checkpoint(\n",
        "    os.path.join(checkpoint_path, \"last.ckpt\")).to(device)\n",
        "model.eval()\n",
        "data_mean = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_mean.npy\"))\n",
        "data_std = np.load(\n",
        "    os.path.join(checkpoint_path, \"data_std.npy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0fzadC0Npiu"
      },
      "source": [
        "# Audio Input\n",
        "\n",
        "You now have a few options for getting source audio into the model.\n",
        "Whichever you choose, monophonic audio will give you best results. Polyphony is likely to result in chaos.\n",
        "\n",
        "You only need to run one of these cells. Whichever one you ran last will be used as the model input. When you're done, jump down to **Prepare Audio** below.\n",
        "\n",
        "To start with, why not jump in with the pre-populated YouTube URL?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSihCDrj7clT",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Get Audio from YouTube\n",
        "\n",
        "#@markdown It's hard to beat the default video link...\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=dYvPCgcFDIo\" #@param {type: \"string\"}\n",
        "start_in_seconds = 6.5 #@param {type: \"number\"}\n",
        "length_in_seconds = 20.0 #@param {type: \"number\"}\n",
        "\n",
        "!rm *.wav\n",
        "!youtube-dl --extract-audio --audio-format wav {youtube_url} #-o yt_audio.wav\n",
        "!mv *.wav yt_audio.wav\n",
        "\n",
        "rate, audio = wavfile.read(\"yt_audio.wav\")\n",
        "audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "audio = audio[int(rate * start_in_seconds):int(rate * (start_in_seconds + length_in_seconds))]\n",
        "audio = resample_audio(audio, rate, model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaMg2gA7Rdwv"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8w09hkUJOorz"
      },
      "source": [
        "#@title 2. Upload an audio file\n",
        "#@markdown For now, only .wav files are supported.\n",
        "\n",
        "!rm -rf *.wav\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "rate, audio = wavfile.read(file_name)\n",
        "audio = convert_to_float32_audio(make_monophonic(audio))\n",
        "audio = resample_audio(audio, rate, model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv07B0plRfkX"
      },
      "source": [
        "OR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2_zxLaRgfn",
        "cellView": "form"
      },
      "source": [
        "#@title 3. Record audio\n",
        "#@markdown Try singing or whistling into the microphone and becoming an instrument yourself!\n",
        "\n",
        "record_seconds = 10 #@param {type: \"number\"}\n",
        "audio = record_audio(record_seconds, sample_rate=model.sample_rate)\n",
        "ipd.Audio(audio, rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANL8C7JzOGMC"
      },
      "source": [
        "# Prepare Audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxM8OhUR_mJ8",
        "cellView": "form"
      },
      "source": [
        "#@title Extract Audio Features\n",
        "#@markdown Here we extract F0 using CREPE and A-weighted loudness.\n",
        "\n",
        "use_full_crepe_model = True #@param {type: \"boolean\"}\n",
        "with torch.no_grad():\n",
        "  f0, confidence = extract_f0_with_crepe(\n",
        "      audio,\n",
        "      full_model=use_full_crepe_model,\n",
        "      maximum_frequency=1000)\n",
        "  loudness = extract_perceptual_loudness(audio)\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, True, False, figsize=(12, 5))\n",
        "axs[0].plot(f0)\n",
        "# axs[0].set_title(\"F0\")\n",
        "axs[0].set_ylabel(\"Frequency (Hz)\")\n",
        "axs[1].plot(confidence)\n",
        "# axs[1].set_title(\"Pitch Confidence\")\n",
        "axs[1].set_ylabel(\"Confidence\")\n",
        "axs[2].plot(loudness)\n",
        "axs[2].set_ylabel(\"Loudness\")\n",
        "\n",
        "axs[2].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: (x * model.control_hop / model.sample_rate)))\n",
        "axs[2].set_xlabel(\"Time (seconds)\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30CPJ2CQHJu2",
        "cellView": "form"
      },
      "source": [
        "#@title Adjust Control Signals { run: \"auto\" }\n",
        "#@markdown Our source audio might not quite match the characteristics of the training audio, so let's adjust it to fit\n",
        "\n",
        "#@markdown Play with these parameters to shift pitch and loudness into an appropriate range:\n",
        "octave_shift = 1 #@param {type: \"slider\", min: -4, max: 4, step: 1}\n",
        "loudness_scale = 0.5 #@param {type: \"slider\", min: 0, max: 2, step: 0.01}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown These parameters are slightly more experimental:\n",
        "loudness_floor = 0 #@param {type: \"slider\", min: 0, max: 1, step: 0.01}\n",
        "loudness_conf_filter = 0 #@param {type: \"slider\", min: 0, max: 0.5, step: 0.001}\n",
        "pitch_conf_filter = 0 #@param {type: \"slider\", min: 0, max: 0.5, step: 0.001}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown These parameters are very experimental but can make some fun wacky sounds:\n",
        "pitch_smoothing = 0 #@param {type: \"slider\", min: 0, max: 100}\n",
        "loudness_smoothing = 0 #@param {type: \"slider\", min: 0, max: 100}\n",
        "\n",
        "with torch.no_grad():\n",
        "  f0_filtered = f0 * (confidence > pitch_conf_filter)\n",
        "  loudness_filtered = loudness * (confidence > loudness_conf_filter)\n",
        "  f0_shifted = f0_filtered * (2 ** octave_shift)\n",
        "  loudness_floored = loudness_filtered * (loudness_filtered > loudness_floor) - loudness_floor\n",
        "  loudness_scaled = loudness_floored * loudness_scale\n",
        "  # loudness = loudness * (confidence > 0.4)\n",
        "  \n",
        "  loud_norm = (loudness_scaled - data_mean[1]) / data_std[1]\n",
        "  \n",
        "  f0_t = torch.tensor(f0_shifted, device=device).float()\n",
        "  loud_norm_t = torch.tensor(loud_norm, device=device).float()\n",
        "\n",
        "  if pitch_smoothing != 0:\n",
        "    f0_t = torch.nn.functional.conv1d(\n",
        "      f0_t.expand(1, 1, -1),\n",
        "      torch.ones(1, 1, pitch_smoothing * 2 + 1, device=device) /\n",
        "        (pitch_smoothing * 2 + 1),\n",
        "      padding=pitch_smoothing\n",
        "    ).squeeze()\n",
        "  f0_norm_t = torch.tensor((f0_t.cpu() - data_mean[0]) / data_std[0], device=device).float()\n",
        "\n",
        "  if loudness_smoothing != 0:\n",
        "    loud_norm_t = torch.nn.functional.conv1d(\n",
        "      loud_norm_t.expand(1, 1, -1),\n",
        "      torch.ones(1, 1, loudness_smoothing * 2 + 1, device=device) /\n",
        "        (loudness_smoothing * 2 + 1),\n",
        "      padding=loudness_smoothing\n",
        "    ).squeeze()\n",
        "  f0_norm_t = torch.tensor((f0_t.cpu() - data_mean[0]) / data_std[0], device=device).float()\n",
        "  \n",
        "  control = torch.stack((f0_norm_t, loud_norm_t), dim=0)\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, True, False, figsize=(12, 5))\n",
        "axs[0].plot(f0_norm_t.cpu())\n",
        "# axs[0].set_title(\"F0\")\n",
        "axs[0].set_ylabel(\"Adjusted Frequency\")\n",
        "axs[1].plot(loud_norm_t.cpu())\n",
        "axs[1].set_ylabel(\"Adjusted Loudness\")\n",
        "\n",
        "axs[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: (x * model.control_hop / model.sample_rate)))\n",
        "axs[1].set_xlabel(\"Time (seconds)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-4FbaTOImG"
      },
      "source": [
        "# Generation Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBq3VDPz_u9h",
        "cellView": "form"
      },
      "source": [
        "#@title Synthesise Audio!\n",
        "#@markdown Finally, run this cell to get some audio from the model.\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  out = model(f0_t.expand(1, 1, -1), control.unsqueeze(0))\n",
        "  run_time = time.time() - start_time\n",
        "rtf = (audio.shape[-1] / model.sample_rate) / run_time\n",
        "print(\"Audio generated in %.2f seconds. That's %.1f times faster than the real time threshold!\" % (run_time, rtf))\n",
        "ipd.Audio(out.detach().cpu().numpy(), rate=model.sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}